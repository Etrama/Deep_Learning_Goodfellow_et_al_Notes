{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Norm - size of a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/aj97mdGD6kK0EQ39kmT_p0LRinlBmmvwo7aTiW2sHUs.original.fullsize.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.norm.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same example as given in the link along with a small exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.arange(9) - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4, -3, -2, -1,  0,  1,  2,  3,  4])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape((3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4, -3, -2],\n",
       "       [-1,  0,  1],\n",
       "       [ 2,  3,  4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.745966692414834"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.745966692414834"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So, the L1 norm just seems to be the summation of all the absolute values of the elements in a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for any vector of any shape, we would just flatten the vector, and add the absolute values of the all the elements to get the L1 norm:    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html<br>\n",
    "https://stackoverflow.com/questions/18777737/how-to-calculate-the-absolute-value-for-an-array-in-python<br>\n",
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.sum.html<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_norm_calculator(vector):\n",
    "    x = vector.flatten()\n",
    "    x = np.absolute(x)\n",
    "    x = np.sum(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_calculator(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_norm_calculator(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(b, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the above value 7? why is it not 20? Are vector norms and matrix norms calculated differently?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we guessed right. Check out the special cases here: https://en.wikipedia.org/wiki/Matrix_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the matrix norm is basically the sum of all the elements of a column which is why it comes out to 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's extend this to the L2 Norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.745966692414834"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(a,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.3484692283495345"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(b,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm_calculator_vec(vector):\n",
    "    x = vector.flatten()\n",
    "    x = np.absolute(x)* np.absolute(x) #Elementwise product\n",
    "    x = np.sum(x)\n",
    "    return np.sqrt(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://www.sharpsightlabs.com/blog/numpy-square-root/<br>\n",
    "https://stackoverflow.com/questions/13567345/how-to-calculate-the-sum-of-all-columns-of-a-2d-numpy-array-efficiently<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.745966692414834"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_norm_calculator_vec(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_norm_calculator_matrix(matrix):\n",
    "    x = np.absolute(matrix) * np.absolute(matrix) #elementwise product\n",
    "    x = x.sum(axis=0)\n",
    "    x = np.sqrt(x)\n",
    "    return max(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.58257569495584"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_norm_calculator_matrix(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If np.sqrt(21) was the answer then the above is the right answer. It seems like we were fooled again by the definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://math.stackexchange.com/questions/3044929/l2-norm-of-a-matrix-is-this-statement-true<br>\n",
    "Seems like a complicated story, remains to be seen if we will actually need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The grand takeaway here is calculation of L<sup>p</sup> norms for vectors is trivially simple based on the formula and the two functions we just went over, the matrix norm however, is defined differently and will need to be handled as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In general, a norm is function which satisfies the following properties:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/lRDkl4gvC9MMUmfq8usGICW4CwS_IQ6OEx_CRYhNlr4.original.fullsize.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The norm is zero when the input to the norm is zero, trivial really, when you think about completely zero vectors.\n",
    "2. The norm of the addition of two vectors will be less than or equal to the sum of the individual norms of these vectors.\n",
    "3. The norm of any real constant multiplied by the input vector will be the same as the product of the absolute value of this constant and the norm of the input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 and L1 norms are very frequently used in ML."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 norm is often mentioned without the subsccript 2. \n",
    "### L2 norm is used to measure the size of a vector by using- x<sup>T</sup>x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared L2 norm is also quite frequently used. <br>This is because the squared L2 norm is simply the sum of the squares of the individual elements (no needs for absolute either, since the square of a negative number will be the same as the square of a positive number. <br>Therefore:<br>\n",
    "squared_L2_norm_of_vector(x) = x<sub>1</sub><sup>2</sup> + x<sub>2</sub><sup>2</sup> + x<sub>3</sub><sup>2</sup> + ..... + x<sub>n</sub><sup>2</sup><br>\n",
    "squared_L2_norm_of_vector = F<br>\n",
    "Derivative of F(x) wrt x<sub>1</sub> = 2X<sub>1</sub><br>\n",
    "Derivative of F(x) wrt x<sub>2</sub> = 2X<sub>2</sub><br>\n",
    "......<br>\n",
    "Dervative of F(x) wrt x<sub>n</sub> = 2X<sub>n</sub><br>\n",
    "\n",
    "The partial derivatives depend only on the corresponding element of x, whereas for the non-squared L2 norm they depend on the entire vector.\n",
    "\n",
    "L2_norm_of_vector(x) = np.sqrt(x<sub>1</sub><sup>2</sup> + x<sub>2</sub><sup>2</sup> + x<sub>3</sub><sup>2</sup> + ..... + x<sub>n</sub><sup>2</sup>)<br>\n",
    "L2_norm_of_vector = G<br>\n",
    "Derivative of G(x) wrt x<sub>1</sub> = 0.5 . G(x) ^ (-1/2) . 2 . x<sub>1</sub><br>\n",
    "Derivative of G(x) wrt x<sub>1</sub> = 0.5 . 2 . x<sub>1</sub> . (np.sqrt(x<sub>1</sub><sup>2</sup> + x<sub>2</sub><sup>2</sup> + x<sub>3</sub><sup>2</sup> + ..... + x<sub>n</sub><sup>2</sup>)) ^ (-1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The squared L2 norm grows very slowly near the origin since the square of a number less than 1 will be less than the original number itself. In these cases it is better to use the L1 norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010000000000000002"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0.1\n",
    "math.pow(a,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 norm is often used as a substitute to count the number of non zero entries in a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Norm - simply the largest element in the vector:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/gDZeq2S8-Zvh1c6go5C_JDUeR-Mx7IFSWmMSPa3PX60.original.fullsize.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Frobenius norm is analogous to the L2 norm and is used to measure the size of a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/2HvnO-qWnslr46ZC-k28_0jMVtThgaTSGRa8mRCRDc4.original.fullsize.png\"><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4, -3, -2],\n",
       "       [-1,  0,  1],\n",
       "       [ 2,  3,  4]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.745966692414834"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(b, 'fro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4, -3, -2, -1,  0,  1,  2,  3,  4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(9) - 4\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid norm order for vectors.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2289\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2290\u001b[1;33m                 \u001b[0mord\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2291\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not int",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-038cedf71331>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(x, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m   2290\u001b[0m                 \u001b[0mord\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2291\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2292\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Invalid norm order for vectors.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2293\u001b[0m             \u001b[0mabsx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2294\u001b[0m             \u001b[0mabsx\u001b[0m \u001b[1;33m**=\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid norm order for vectors."
     ]
    }
   ],
   "source": [
    "np.linalg.norm(a, 'fro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looks like 'fro' doesnt work for 1D vectors. Why arent we using the .shape argument for all of these? Is it somehow less efficient? I guess it would just traverse an array and increase the count right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/CXaVFkAMnDbaJQ-dzW7Up1rrugQEmfZKx0HM66IkaQo.original.fullsize.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special kinds of Matrices and Vectors:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagonal Matrix: All the non-diagonal elements are zero, for example in the identity matrix. A matrix where only the diagonal elements are non-zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.eye(4,4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is to find the diagonal of a matrix.\n",
    "np.diagonal(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://docs.scipy.org/doc/numpy/reference/generated/numpy.diagonal.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Diagonal matrices are computationaly efficient.<br>\n",
    "diag(v)x = Hadamard_product(v,x)<br>\n",
    "2. The inverse only exists if the diagonal elements are non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Symmetric Matrix: A matrix that is equal to its own transpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit Vector : A vector with unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### x and y are orthogonal if x<sup>T</sup>y = 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for non-zero norm, x and y are at 90 degrees to each other.<br>\n",
    "If the vectors are not only orthogonal, but also have unit norm, then we call them <b>Orthonormal</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the book:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/WtDM_cHOcsQN_hTraMXVDrZMQqaEaemOvj39RGtnA68.original.fullsize.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing mathjax\n",
    "<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n",
    "<script id=\"MathJax-script\" async src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n",
    "\n",
    "Example (trial):\n",
    "When $$(a \\ne 0),$$ there are two solutions to \\(ax^2 + bx + c = 0\\) and they are\n",
    "$$x = {-b \\pm \\sqrt{b^2-4ac} \\over 2a}.$$\n",
    "(end of trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigendecomposition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to prime factorization of integers, matrices can be decomposed to show some of their functional properties that are not obvious from their representation as an array of elements. One of these ways of representation is called <b>Eigendecomposition</b>, in which we decompose a matrix into <b>eigenvalues</b> and <b>eigenvectors</b> ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://docs.mathjax.org/en/latest/basic/mathematics.html<br>\n",
    "https://oeis.org/wiki/List_of_LaTeX_mathematical_symbols<br>\n",
    "https://tex.stackexchange.com/questions/327844/real-number-symbol-r-not-working<br>\n",
    "https://tex.stackexchange.com/questions/52276/inline-equation-in-latex-with-text<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{A} \\boldsymbol{v}=\\lambda \\boldsymbol{v}$$\n",
    "where, <br>\n",
    "$$\\boldsymbol{A} = \\text{Square matrix A, mutiplication by A only alters the scale of v}$$<br>\n",
    "$$\\boldsymbol{v} = \\text{Eigenvector of Square matrix A}$$<br>\n",
    "$$\\lambda = \\text{Eigenvalue of the eigenvector}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the book:<img src=\"https://cdn.mathpix.com/snip/images/dJePmPkLeEQkTLqtcg16K6Cz9Yu7--99LvcQXU2FUn4.original.fullsize.png\"><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e from above, for $$s \\in \\mathbb{R}, s \\neq 0,$$\n",
    "$$\\boldsymbol{A} \\boldsymbol{s}=\\lambda \\boldsymbol{s}$$\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{A} \\text{ has n linearly dependent eigenvectors:}\\left\\{\\boldsymbol{v}^{(1)}, \\ldots,\\right., \\boldsymbol{v}^{(n)}\\} \\text{ with eigenvalues: } \\left\\{\\lambda_{1}, \\ldots, \\lambda_{n}\\right\\}, \\text{ then we can concatenate (just place them side by side, nothing special) all the eigenvectors to form a matrix } \\boldsymbol{V} \\text{ and we can do the same for } \\lambda, i.e: $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{V}=\\left[\\boldsymbol{v}^{(1)}, \\ldots\\right.,\\boldsymbol{v}^{(n)}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{\\lambda}=\\left[\\lambda_{1}, \\ldots,\\right., \\lambda_{n}]^{\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>eigendecomposition</b> of $\\boldsymbol{A}$ is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{A}=\\boldsymbol{V} \\operatorname{diag}(\\boldsymbol{\\lambda}) \\boldsymbol{V}^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Didn't really undestand how we ended up with the above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://en.wikipedia.org/wiki/Eigendecomposition_of_a_matrix <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn.mathpix.com/snip/images/8vTLc7NdfONC5iK_bFgQ0AZ_DCsN1tsc9hHloYHj1Ys.original.fullsize.png\"><br>\n",
    "<img src=\"https://cdn.mathpix.com/snip/images/CaG7TiktZTejvokkdVoHb-rKfL6hTpXrIJR5Q0icEII.original.fullsize.png\"><br>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doubt:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\boldsymbol{A} \\boldsymbol{v}=\\lambda \\boldsymbol{v}$$\n",
    "We replace $\\boldsymbol{v}$  by $\\boldsymbol{Q}$, and $\\lambda$ by its diagonal version????? Ordering seems off\n",
    "$$\\boldsymbol{A} \\boldsymbol{Q}=\\boldsymbol{Q} \\Lambda$$\n",
    "<br>\n",
    "I'm still not clear on how we went from $\\boldsymbol{A} \\boldsymbol{v}=\\lambda \\boldsymbol{v}$ to $\\boldsymbol{A} \\boldsymbol{Q}=\\boldsymbol{Q} \\Lambda$\n",
    "\n",
    "The second step is clear enough, multiply by the inverse matrix on both the sides (Q inverse), and since the inverse of a symmetric matrix is the same as its transpose, we get the result below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For real symmetric matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\boldsymbol{A}=\\boldsymbol{Q} \\boldsymbol{\\Lambda} \\boldsymbol{Q}^{\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some easy pickings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A matrix whose eigenvalues are all positive is called <b>positive definite</b>\n",
    "* A matrix whose eigenvalues are all positive or zero is called <b>positive semidefinite</b>\n",
    "* A matrix whose eigenvalues are all negative is called <b>negative definite</b>\n",
    "* A matrix whose eigenvalues are all negative or zero is called <b>negative semidefinite</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For positive semidefinite matrices: <br>\n",
    "For all input x, x<sup>T</sup>Ax will be greater than or equal to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positive Definite matrices:<br>\n",
    "For all input x, x<sup>T</sup>Ax will be greater than 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Alternate way to factorize a matrix rather than doing it with eigenvalues and eigenvectors.\n",
    "* Generally more applicable. Every matrix has an SVD, every matrix does not have an eigenvalue and eigenvector.(non-square matrices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = V . diag(&lambda;) . V<sup>-1</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need an intuitive way to think about Eigenvalues and eigenvectors, this equation aint gonna cut it.<br>\n",
    "1. https://math.stackexchange.com/questions/243533/how-to-intuitively-understand-eigenvalue-and-eigenvector\n",
    "2. http://setosa.io/ev/eigenvectors-and-eigenvalues/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVD says: A = U . D . V<sup>T</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* U (left singular vector) and V (right singular vector) need to be orthogonal matrices (U<sup>T</sup> = U<sup>-1</sup>, same for V)\n",
    "* D (singular values) need not be square"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some practice with eigenvalues and SVD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.physics.utah.edu/~detar/lessons/python/numpy_eigen/node1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lhs - left hand side, \n",
    "rhs - right hand side\n",
    "\n",
    "We are trying to get this result:\n",
    "$$\\boldsymbol{A} \\boldsymbol{v}=\\lambda \\boldsymbol{v}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a few matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "B = np.array([[9,9,9],[9,9,9],[9,9,9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, vectors = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.61168440e+01, -1.11684397e+00, -9.75918483e-16])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23197069, -0.78583024,  0.40824829],\n",
       "       [-0.52532209, -0.08675134, -0.81649658],\n",
       "       [-0.8186735 ,  0.61232756,  0.40824829]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = np.dot(A,vectors)\n",
    "rhs = np.dot(values,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73863537e+00,  8.77649763e-01, -3.88578059e-16],\n",
       "       [-8.46653421e+00,  9.68877101e-02, -3.33066907e-16],\n",
       "       [-1.31944331e+01, -6.83874343e-01, -7.21644966e-16]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -3.15193256, -12.56821563,   7.49157328])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mistake we made here was to use vector multplication instead of matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = A * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.23197069, -1.57166048,  1.22474487],\n",
       "       [-2.10128837, -0.4337567 , -4.89897949],\n",
       "       [-5.7307145 ,  4.89862048,  3.67423461]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = values * vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73863537e+00,  8.77649763e-01, -3.98417052e-16],\n",
       "       [-8.46653421e+00,  9.68877101e-02,  7.96834105e-16],\n",
       "       [-1.31944331e+01, -6.83874343e-01, -3.98417052e-16]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False],\n",
       "       [False, False, False],\n",
       "       [False, False, False]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs == rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's break it down:\n",
    "A - 3x3 matrix<br>\n",
    "vectors - 3x3 matrix<br>\n",
    "values - 3x1 or 1x3 (depends- (3,)) matrix<br>\n",
    "\n",
    "A x vectors - 3x3 matrix<br>\n",
    "values x vectors - NOT 3x3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.array([values,values,values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.61168440e+01, -1.11684397e+00, -9.75918483e-16],\n",
       "       [ 1.61168440e+01, -1.11684397e+00, -9.75918483e-16],\n",
       "       [ 1.61168440e+01, -1.11684397e+00, -9.75918483e-16]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73863537e+00,  8.77649763e-01, -3.98417052e-16],\n",
       "       [-8.46653421e+00,  9.68877101e-02,  7.96834105e-16],\n",
       "       [-1.31944331e+01, -6.83874343e-01, -3.98417052e-16]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values * vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we didnt need to this sort of broadcasting, compare the above to the rhs and we see why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, comparing the old lhs and new rhs it seems like we need to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2,3],[4,5,6],[7,8,9]])\n",
    "values, vectors = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = np.dot(A,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73863537e+00,  8.77649763e-01, -3.88578059e-16],\n",
       "       [-8.46653421e+00,  9.68877101e-02, -3.33066907e-16],\n",
       "       [-1.31944331e+01, -6.83874343e-01, -7.21644966e-16]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = values*vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73863537e+00,  8.77649763e-01, -3.98417052e-16],\n",
       "       [-8.46653421e+00,  9.68877101e-02,  7.96834105e-16],\n",
       "       [-1.31944331e+01, -6.83874343e-01, -3.98417052e-16]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False],\n",
       "       [False, False, False],\n",
       "       [False, False, False]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs == rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird - columns 1 and 2 seem pretty much the same to me in the lhs and rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhs = np.multiply(values,vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.73863537e+00,  8.77649763e-01, -3.98417052e-16],\n",
       "       [-8.46653421e+00,  9.68877101e-02,  7.96834105e-16],\n",
       "       [-1.31944331e+01, -6.83874343e-01, -3.98417052e-16]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is befuddling. Is this one of those matrices which does not have an eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://math.stackexchange.com/questions/651934/invertibility-eigenvalues-and-singular-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like all sqaure matrices with a non-zero determinant should be invertible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.51619735392994e-16"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, such luck, much wow, just had to pick a matrix with a zero determinant and doubt everything I just learned.<br> Anyway, we just need to pick a matrix with a non-zero determinant and we should be fine.<br>It's still pretty interesting that 2 of the columns turn out to be the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-11.999999999999957"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,20,3],[4,15,6],[7,8,9]])\n",
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we should be set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, vectors = np.linalg.eig(A)\n",
    "#values and vectors in alphabetical order - FYI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.57317948, -0.51726151,  0.94408203])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.57506513,  0.79245773, -0.69702376],\n",
       "       [ 0.59331453,  0.03125058, -0.10446092],\n",
       "       [ 0.56327432, -0.60912572,  0.70939819]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "lhs = np.dot(A,vectors)\n",
    "rhs = values*vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.13117868, -0.40990788, -0.6580476 ],\n",
       "       [14.57962441, -0.01616472, -0.09861968],\n",
       "       [13.84144107,  0.31507729,  0.66973009]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.13117868, -0.40990788, -0.6580476 ],\n",
       "       [14.57962441, -0.01616472, -0.09861968],\n",
       "       [13.84144107,  0.31507729,  0.66973009]])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False],\n",
       "       [False, False, False],\n",
       "       [False, False, False]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhs == rhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, a couple of takeaways:<br>\n",
    "1. the lhs==rhs doesnt seems to work\n",
    "2. we use a dot product on one side and * on the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(lhs,rhs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we dealt with the first takeaway. Use np.allclose() to compare numpy arrays. <br>https://docs.scipy.org/doc/numpy/reference/generated/numpy.allclose.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigating the second takeaway:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,1],[1,1]])\n",
    "B = A + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A*B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4, 4],\n",
       "       [4, 4]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2],\n",
       "       [2, 2]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.13117868, -0.40990788, -0.6580476 ],\n",
       "       [14.57962441, -0.01616472, -0.09861968],\n",
       "       [13.84144107,  0.31507729,  0.66973009]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[1,20,3],[4,15,6],[7,8,9]])\n",
    "np.dot(A,vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I see now:<br>\n",
    "\\* - Hadamard Product<br>\n",
    "np.dot() - row+column song product - https://www.youtube.com/watch?v=BGbiHdKHG7o - actually heard Jeremy mention it while doing Fast AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so the lhs makes sense now - we are supposed to use np.dot() for the lhs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the rhs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.35605708,  18.88197677, -16.40432616])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(values,vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took the dot product of a 1x3 matrix and a 3x3 matrix to get a 1x3 matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we need to broadcast the multiplication by column, the values array has 3 columns and each column must be multiplied down like mentioned in one of the links. So we need the inplace product / Hadamard product here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14.13117868, -0.40990788, -0.6580476 ],\n",
       "       [14.57962441, -0.01616472, -0.09861968],\n",
       "       [13.84144107,  0.31507729,  0.66973009]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values*vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all is right with the world. Moving on.....<br>\n",
    "We still need some physical intuition about eigenvalues and vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.999999999999996"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[5,0],[0,5]])\n",
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5, 0],\n",
       "       [0, 5]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, vectors = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a7cf8226d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.quiver([0,0],[0,0],A[0],A[1],angles = 'xy', scale_units='xy', scale = 1)\n",
    "plt.xlim(-10, 10)\n",
    "plt.ylim(-10, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is getting out of hand. Let's just explore with the setosa link given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[2,0],[0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, vectors = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woah, nice. Now that's starting to make sense. So, the values are what you would use to ideally scale your identity matrix. But, what if the vector isnt the identity matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.875"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[2.75,0],[0,2.5]])\n",
    "np.linalg.det(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, vectors = np.linalg.eig(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.75, 2.5 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we say that the eigenvalues are the magnitude of the projections of the original vectors along the primary axes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, I think  I figured it out, so the A . v multiplication on the LHS, it rotates the vectors.<br>\n",
    "On the rhs, we the projections along the v vectors scaled by certain amounts to get the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, lets say we have two vectors and we multiply them.<hr>\n",
    "The product is a scalar. It's the projection on A onto v and v onto A. (dot product)<hr>\n",
    "Consider the vector v and the result np.dot(A,v), we can interpret this as a scaled version of v along the same direction of v (assuming the direction of course, since dot product gives a scalar result)<hr>\n",
    "Instead of all this jazz, we could simply have scaled v by some factor, this factor being &lambda; <hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in a way it's about trying to find a vector (v) that is similar to another.(A)<br>Seems like it might be easier to deal with v than it is to deal with A.<br>Will v have lesser dimensions or something? Absolutely not. For all the matrices that we have seen till now, v has the same dimensions as A.<br>Ah wait, the final form was -<br>\n",
    "A = V . diag(&lambda;) . V<sup>-1</sup>; so &lambda; (eigenvalue) IS easier to deal with since it is a diagonal vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A = U . D . V<sup>T</sup>; where U and V are orthogonal, i.e their inverse is their transpose. D is a diagonal matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.svd.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random([4,4])*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.37688405, 0.81912202, 2.59954613, 0.53429634],\n",
       "       [1.27958808, 9.80734046, 7.52157338, 8.50465961],\n",
       "       [2.49378264, 7.07544599, 6.24251801, 9.45278165],\n",
       "       [4.02867635, 1.61368105, 3.45767497, 2.88372876]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, s, vh = np.linalg.svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.13109882,  0.65039441,  0.40758922, -0.62743226],\n",
       "       [-0.71119983, -0.33467005,  0.58991685,  0.18490231],\n",
       "       [-0.64441634, -0.02569249, -0.68571474, -0.33743558],\n",
       "       [-0.24846625,  0.68141248, -0.12515762,  0.67696168]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.09662243e+01, 5.06242202e+00, 2.08371132e+00, 8.25505887e-03])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.18891207, -0.57439255, -0.50424126, -0.61654374],\n",
       "       [ 0.878865  , -0.36181775,  0.27046381, -0.15340646],\n",
       "       [-0.03983804,  0.51143295,  0.37592006, -0.77170809],\n",
       "       [ 0.43625849,  0.52687517, -0.72888558, -0.02840591]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = np.dot(u*s,vh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.37688405, 0.81912202, 2.59954613, 0.53429634],\n",
       "       [1.27958808, 9.80734046, 7.52157338, 8.50465961],\n",
       "       [2.49378264, 7.07544599, 6.24251801, 9.45278165],\n",
       "       [4.02867635, 1.61368105, 3.45767497, 2.88372876]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(result,A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U and S are Hadamard products (broadcasted) and we then take a dot product with the inverse of v."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont feel like going into the physical significance of SVD myself right now.....<br>https://medium.com/data-science-group-iitr/singular-value-decomposition-elucidated-e97005fb82fa - is pretty good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moore Penrose PseudoInverse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used to find the \"pseudoinverse\" of matrices using SVD where there might not be a real inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A - matrix<br>\n",
    "Ax = y might not have a solution of the form x = By, since B may not exist esp for non-square matrices.<br>\n",
    "So, we calculate the pseudoinverse:\n",
    "A<sup>+</sup> = V . D<sup>+</sup> . U<sup>T</sup>\n",
    "\n",
    "U,D,V - SVD of a A.\n",
    "D<sup>+</sup> = pseudoinverse of D - take reciprocal of the non-zero elements then take the transpose of the matrix.\n",
    "\n",
    "Two cases:<br>\n",
    "1. If A has more columns than rows:<br>\n",
    "MPP provides one of many possible solutions<br>\n",
    "x = A<sup>+</sup>y  where x has the least L2 norm possible.\n",
    "2. If A has more rows than columns:\n",
    "Possible that there is no solution.\n",
    "Gives closest solution such that the L2 norm of (Ax-y) is as close to zero as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trace:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of all diagonal entries of a matrix.<br>\n",
    "Read the book for this part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinant:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usual determinant solution of a square matrix.<br>\n",
    "Also the product of all eigenvalues<br>\n",
    "Measure of contraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Principal Component Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We want to apply lossy compression - i.e say project these points like a projection of a 3D vector on a 2D surafe.<br>\n",
    "* PCA is used for visualizing the data and transformations in a human interpretable format, project from N dimendions to 2 or 3 dimensions.<br>\n",
    "* We lose some information in the course of this projection, trying to keep this loss to a minimum.<br>\n",
    "Say we have m points:<br>\n",
    "{x<sup>1</sup>, x<sup>2</sup>,......, x<sup>m</sup>}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all points:<br> \n",
    "x<sup>(i)</sup> $\\subset \\mathbb{R}$<sup>n</sup>,<br>\n",
    "Find a corresponding point <br>\n",
    "c<sup>(i)</sup> $\\subset \\mathbb{R}$<sup>l</sup>,<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For l < n, we can store a representation of points in a lower dimensional space. (less data, simpler math)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to find a function and its inverse such that:<br>\n",
    "$f(x) = c$, c is a l-dimensional point<br>\n",
    "$x \\approx g(f(c))$, x is a n-dimensional point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let: $g(c) = Dc$, where $D \\subset \\mathbb{R}$<sup>n $\\times$ l</sup>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is defined by our choice of the function $g$.<br>\n",
    "For simplicity, the columns of $D$ are orthogonal to each other. (Inverse of Matrix = Transpose of Matrix, only square matrices can be orthogonal, so this is an approximation)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a unique solution, we constrain all columns of $D$ to have a unit norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source (math jax exponentiation): https://math.meta.stackexchange.com/questions/5020/mathjax-basic-tutorial-and-quick-reference<br>\n",
    "argmin: https://stackoverflow.com/questions/36174987/how-to-typeset-argmin-and-argmax-in-markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the optimal code point $c^*$ for input $x$,<br>\n",
    "minimize the distance between the input $x$ and what the decoder predicts $x$ to be, i.e $g(c^*)$<br>\n",
    "We can define the distance using L2 norm, so:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c^* = \\underset{c}{\\operatorname{arg min}} \\Vert x - g(c)\\Vert_2$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think of it like $x- g(c)$ being the distance function between the input and its projection.<br> We take the argmin just for the special case where we want to minimize the distance between them.<br> We want the argument - the the input to the function - $c^*$ when the value of the distance function is the least.<br>\n",
    "Squaring the L2 norm because that won't change the arg min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c^* = \\underset{c}{\\operatorname{arg min}} \\Vert x - g(c)\\Vert_2^2$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared L2 norm of a vector $x$ is also given by $x^Tx$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so, the rhs without the argmin is:<br>\n",
    "$(x - g(c))^T(x- g(c))$<br><br>\n",
    "$= x^Tx - x^Tg(c) - g(c)^Tx + g(c)^Tg(c)$<br><br>\n",
    "$= x^Tx - 2x^Tg(c) + g(c)^Tg(c)$<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The book says, the scalar $g(c)^Tx$ is equal to its own transpose. Let's work it out:<br>\n",
    "$g(c)$ - Inverse function, gives a point $y$ in N-D space\n",
    "$x$ - is a point in N-D space <br>\n",
    "$y$ is Nx1\n",
    "$y^T$ is 1xN\n",
    "$x$ is Nx1<br>\n",
    "$y^Tx$ is a 1xN $\\times$ Nx1 vector<br>\n",
    "Which is a scalar, and a scalar is its own transpose.<br>\n",
    "Bringing back the arg min:<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c^* = \\underset{c}{\\operatorname{arg min}} - 2x^Tg(c) + g(c)^Tg(c)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since, $= x^Tx$ does not vary with $c$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substitute, $g(c) = Dc$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c^* = \\underset{c}{\\operatorname{arg min}} - 2x^TDc + c^TD^TDc$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$D^TD = I_l$, since it is roughly orthogonal, tranpose being inverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$c^* = \\underset{c}{\\operatorname{arg min}} - 2x^TDc + c^TI_lc$ <br>\n",
    "$c^* = \\underset{c}{\\operatorname{arg min}} - 2x^TDc + c^Tc$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In normal calculus, wherever the derivative is 0. You either get a minima or a maxima of a function. We can key in the obtained value in the function to check the point obtained is a minima or a maxima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For vector calculus, we use gradients for this purpose:<br>\n",
    "Source: https://arxiv.org/pdf/1802.01528.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient is just a collection of partial derivatives. For the gradient with respect to c for our purpose here, we just need to take the partial derivative of the function with respect to c."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small demonstration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,4,5],[5,6,7],[6,7,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 5],\n",
       "       [5, 6, 7],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9999999999999984"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 51,  63,  73],\n",
       "       [ 77, 105, 123],\n",
       "       [ 89, 122, 143]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 16, 25],\n",
       "       [25, 36, 49],\n",
       "       [36, 49, 64]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1, 20, 30],\n",
       "       [20, 36, 49],\n",
       "       [30, 49, 64]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose()*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 62,  76,  88],\n",
       "       [ 76, 101, 118],\n",
       "       [ 88, 118, 138]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a.transpose(),a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for non orthogonal matrices, the multiplication works out like shown in the 4 cells above.<br>\n",
    "Also, note that for non orthogonal matrices there is NOTHIGN SPECIAL about $a^Ta$. It's the normal row,col song product. I was mis-led by the gradient of $c^Tc$ wrt c being $2c$. Was expecting a sqaure somewhere.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For orthogonal matrices:<br>\n",
    "Source: https://en.wikipedia.org/wiki/Orthogonal_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,-0.80,-0.60],[0.80,-0.36,0.48],[0.60,0.48,-0.64]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  , -0.8 , -0.6 ],\n",
       "       [ 0.8 , -0.36,  0.48],\n",
       "       [ 0.6 ,  0.48, -0.64]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.00000000e+00, -4.44089210e-19, -1.42108547e-17],\n",
       "       [ 4.44089210e-19, -2.80000000e-01, -9.60000000e-01],\n",
       "       [ 1.42108547e-17, -9.60000000e-01,  2.80000000e-01]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.    , 0.64  , 0.36  ],\n",
       "       [0.64  , 0.1296, 0.2304],\n",
       "       [0.36  , 0.2304, 0.4096]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    , -0.64  , -0.36  ],\n",
       "       [-0.64  ,  0.1296,  0.2304],\n",
       "       [-0.36  ,  0.2304,  0.4096]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose()*a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+00,  4.44089210e-19,  1.42108547e-17],\n",
       "       [ 4.44089210e-19,  1.00000000e+00, -2.19380070e-17],\n",
       "       [ 1.42108547e-17, -2.19380070e-17,  1.00000000e+00]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a.transpose(),a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell shows that all products in our equation are dot products. I forgot and wanted to confirm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to PCA, consider onlt he RHS,<br>\n",
    "$\\nabla_c(\\underset{c}{\\operatorname{arg min}} - 2x^TDc + c^Tc) = \\nabla_c(- 2x^TDc + c^Tc)  $ , for the case where we find minima, the first term is already the expression for minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set to 0. to find minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_c(- 2x^TDc + c^Tc) = 0 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(- 2D^Tx + 2c) = 0 $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first lhs term - see below.<br>\n",
    "The second one is a bit of a challenge, how does the derivative/gradient come to 2c?<br>Per normal calculus, it makes sense that something like c square when differentiated gives 2c. But how does it actually work with vector calculus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,a.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 2,  4,  6,  8, 10],\n",
       "       [ 3,  6,  9, 12, 15],\n",
       "       [ 4,  8, 12, 16, 20],\n",
       "       [ 5, 10, 15, 20, 25]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a.transpose(),a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2,3,4,5]])\n",
    "a = a.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 2,  4,  6,  8, 10],\n",
       "       [ 3,  6,  9, 12, 15],\n",
       "       [ 4,  8, 12, 16, 20],\n",
       "       [ 5, 10, 15, 20, 25]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a,a.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[55]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(a.transpose(),a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to shoo away some god-awful doubts, I've added the above lines of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check this out:<br>\n",
    "https://atmos.washington.edu/~dennis/MatrixCalculus.pdf<br>\n",
    "According to Proposition 11 in above, shouldn't the gradient be twice of c tranposed? Or is that the difference due to the between derivatives and gradients?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.matrixcalculus.org/<br>\n",
    "Says derivative of transpose of x into x is twice of x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helped:<br>\n",
    "https://math.stackexchange.com/questions/1377764/derivative-of-vector-and-vector-transpose-product<br>\n",
    "https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say c = [c1, c2 ...., cn] - is a column vector<br>\n",
    "Then, $c^Tc $ = [c1, c2 ...., cn]<sup>T</sup>[c1, c2 ...., cn]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is,<br>\n",
    "c1c1 + c2c2 + .... + cncn --->Say this is z."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\nabla_c(f)$ =<br>\n",
    "[<br>\n",
    "partial derivative wrt c1<br>\n",
    "partial derivative wrt c2<br>\n",
    ".<br>\n",
    ".<br>\n",
    ".<br>\n",
    "partial derivative wrt cn<br>\n",
    "]<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above, we get $\\nabla_c(c^Tc) = $ <br>\n",
    "[<br>\n",
    "partial derivative of z wrt c1 ---> 2c1<br>\n",
    "partial derivative of z wrt c2 ---> 2c2<br>\n",
    ".<br>\n",
    ".<br>\n",
    "partial derivative of z wrt cn ---> 2cn<br>\n",
    "]<br>\n",
    "$\\nabla_c(c^Tc) = 2c.$ Finally!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation :$\\nabla_c(- 2x^TDc + c^Tc) = 0 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for the first term:<br>\n",
    "$\\nabla_c(x^TDc) $  <br>\n",
    "http://www.matrixcalculus.org/ says it is transpose of D multiplied by X.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x - nx1 dimensional - col vector<br>\n",
    "D - nxl dimensions <br>\n",
    "c - lx1 dimensions - col vector <br>\n",
    "<hr>\n",
    "x<sup>T</sup> - [x1 x2 ..xn]<br><br>\n",
    "d - <br>\n",
    "[<br>\n",
    "d11 d12 ... d1l<br>\n",
    "d21 d22 ... d2l<br>\n",
    ".<br>\n",
    ".<br>\n",
    "dn1 dn2 ... dnl<br>\n",
    "]<br>\n",
    "<hr>\n",
    "x<sup>T</sup>D-<br>\n",
    "output: (1xn).(nxl) = (1xl) matrix<br>\n",
    "This is a row matrix, im differentiating between elements using commas.<br>\n",
    "[\n",
    "d11x1 + d21x2 + ... + dn1xn,\n",
    "d12x1 + d22x2 + ... + dn2xn,\n",
    "...........,\n",
    "d1lx1 + d2lx2 + ... + dnlxn\n",
    "]<br>\n",
    "<hr>\n",
    "c - [<br>\n",
    "c1<br>\n",
    "c2<br>\n",
    ".<br>\n",
    ".<br>\n",
    "cl<br>\n",
    "]\n",
    "<hr>\n",
    "x<sup>T</sup>Dc- (Let this be function k)<br>\n",
    "output: (1xl).(lxl) = (1x1) matrix - Scalar<br>\n",
    "Output is a scalar - <br>\n",
    "(d11x1 + d21x2 + ... + dn1xn)c1+<br>\n",
    "(d12x1 + d22x2 + ... + dn2xn)c2+<br>\n",
    ".........+<br>\n",
    "(d1lx1 + d2lx2 + ... + dnlxn)cl<br>\n",
    "<hr>\n",
    "Gradient of k with rrespect to c -<br>\n",
    "[<br>\n",
    "partial derivative of k wrt c1<br>\n",
    "partial derivative of k wrt c2<br>\n",
    ".<br>\n",
    ".<br>\n",
    "partial derivative of k wrt cl<br>\n",
    "]<br><br>\n",
    "Which is the following (lx1) vector- <br>\n",
    "[<br>\n",
    "(d11x1 + d21x2 + ... + dn1xn)<br>\n",
    "(d12x1 + d22x2 + ... + dn2xn)<br>\n",
    ".<br>\n",
    ".<br>\n",
    "(d1lx1 + d2lx2 + ... + dnlxn)<br>\n",
    "]<br>\n",
    "This is the transpose of x<sup>T</sup>D --> D<sup>T</sup>x<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It takes rather long to open up every matrix and do the above, I'll consider sticking to the formulas. Maybe. But I also feel very convinced by this result now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, coming back to the original equation, we have<br>\n",
    "$(- 2D^Tx + 2c) = 0 $ <br>\n",
    "$c = D^Tx $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f(x) = c$<br>\n",
    "$f(x) = D^Tx$<br>\n",
    "So the inverse of $f(x)$ is then simply multiplying D to get back x:\n",
    "$g(f(x)) = DD^Tx$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far so good. Now we need to choose D, since we have been hand wavy about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, this is pretty clear in the book itself. Read the last 3 pages of the chapter.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "- [ ] Derive the proof mentioned on Page 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
